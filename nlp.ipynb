{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,os\n",
    "import nltk\n",
    "from nltk import * \n",
    "from nltk.stem import * \n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import matplotlib as plt\n",
    "\n",
    "dir = \"indo/\"\n",
    "listFile = os.listdir(dir)\n",
    "\n",
    "filedir = {}\n",
    "final = []\n",
    "stemmer = StemmerFactory()\n",
    "idn_stemmer = stemmer.create_stemmer()\n",
    "\n",
    "for files in listFile:\n",
    "    # name = os.path.splitext(files)\n",
    "    for i in listFile:\n",
    "        files = open(dir+i,'r').read()\n",
    "\n",
    "        lower = files.lower()\n",
    "\n",
    "        content = idn_stemmer.stem(lower)\n",
    "        final.append(content)\n",
    "    \n",
    "        stopword = set(stopwords.words(\"indonesian\"))\n",
    "\n",
    "        regex =  RegexpTokenizer('\\W+', gaps = True)\n",
    "        for words in final:\n",
    "            tokenizedWords = regex.tokenize(words)\n",
    "\n",
    "        indoToken = []\n",
    "        for words in tokenizedWords:\n",
    "            if words not in stopword:\n",
    "                indoToken.append(words)\n",
    "        wordFreq = FreqDist(indoToken)\n",
    "\n",
    "    for fileName in listFile:\n",
    "        filedir[fileName] = {}  \n",
    "\n",
    "        for terms,freq in wordFreq.most_common(25):\n",
    "            filedir[fileName][terms] = [freq][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'children.txt': {'suzume': 21, 's': 16, 'ta': 16, 'kunci': 14, 'pintu': 13, 'cacing': 11, 'temu': 9, 'batu': 9, 'lihat': 7, 'dunia': 7}, 'kotonoha no niwa (id).txt': {'suzume': 21, 's': 16, 'ta': 16, 'kunci': 14, 'pintu': 13, 'cacing': 11, 'temu': 9, 'batu': 9, 'lihat': 7, 'dunia': 7}, 'weathering.txt': {'suzume': 21, 's': 16, 'ta': 16, 'kunci': 14, 'pintu': 13, 'cacing': 11, 'temu': 9, 'batu': 9, 'lihat': 7, 'dunia': 7}, 'yourname.txt': {'suzume': 21, 's': 16, 'ta': 16, 'kunci': 14, 'pintu': 13, 'cacing': 11, 'temu': 9, 'batu': 9, 'lihat': 7, 'dunia': 7}, 'suzume (id).txt': {'suzume': 21, 's': 16, 'ta': 16, 'kunci': 14, 'pintu': 13, 'cacing': 11, 'temu': 9, 'batu': 9, 'lihat': 7, 'dunia': 7}}\n"
     ]
    }
   ],
   "source": [
    "print(filedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (360, 1), indices imply (5, 149)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/christinaandrea/Desktop/6th Semester/NLP/mini library/nlp.ipynb Cell 4\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m wm \u001b[39m=\u001b[39m tfidf_vector\u001b[39m.\u001b[39mfit_transform(indoToken)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m term \u001b[39m=\u001b[39m tfidf_vector\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vect \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data\u001b[39m=\u001b[39;49mwm,index\u001b[39m=\u001b[39;49mlistFile,columns\u001b[39m=\u001b[39;49mterm)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print(type(term))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/frame.py:797\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    789\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    790\u001b[0m             arrays,\n\u001b[1;32m    791\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    798\u001b[0m             data,\n\u001b[1;32m    799\u001b[0m             index,\n\u001b[1;32m    800\u001b[0m             columns,\n\u001b[1;32m    801\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    802\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    803\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    804\u001b[0m         )\n\u001b[1;32m    805\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    807\u001b[0m         {},\n\u001b[1;32m    808\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    811\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    812\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/internals/construction.py:337\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    333\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[1;32m    334\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/internals/construction.py:408\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    406\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    407\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[0;32m--> 408\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (360, 1), indices imply (5, 149)"
     ]
    }
   ],
   "source": [
    "tfidf_vector = TfidfVectorizer()\n",
    "wm = tfidf_vector.fit_transform(indoToken)\n",
    "term = tfidf_vector.get_feature_names_out()\n",
    "\n",
    "vect = pd.DataFrame(data=wm,index=listFile,columns=term)\n",
    "\n",
    "# print(type(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 122)\t1.0\n",
      "  (1, 53)\t1.0\n",
      "  (2, 93)\t1.0\n",
      "  (3, 40)\t1.0\n",
      "  (4, 119)\t1.0\n",
      "  (5, 145)\t1.0\n",
      "  (6, 1)\t1.0\n",
      "  (7, 135)\t1.0\n",
      "  (8, 17)\t1.0\n",
      "  (9, 19)\t1.0\n",
      "  (10, 66)\t1.0\n",
      "  (11, 86)\t1.0\n",
      "  (12, 73)\t1.0\n",
      "  (13, 84)\t1.0\n",
      "  (14, 24)\t1.0\n",
      "  (15, 8)\t1.0\n",
      "  (16, 122)\t1.0\n",
      "  (17, 100)\t1.0\n",
      "  (18, 115)\t1.0\n",
      "  (19, 54)\t1.0\n",
      "  (20, 132)\t1.0\n",
      "  (21, 93)\t1.0\n",
      "  (22, 97)\t1.0\n",
      "  (23, 24)\t1.0\n",
      "  (24, 29)\t1.0\n",
      "  :\t:\n",
      "  (332, 87)\t1.0\n",
      "  (333, 18)\t1.0\n",
      "  (334, 140)\t1.0\n",
      "  (335, 69)\t1.0\n",
      "  (336, 122)\t1.0\n",
      "  (337, 87)\t1.0\n",
      "  (338, 26)\t1.0\n",
      "  (339, 122)\t1.0\n",
      "  (341, 124)\t1.0\n",
      "  (342, 135)\t1.0\n",
      "  (343, 35)\t1.0\n",
      "  (344, 84)\t1.0\n",
      "  (346, 124)\t1.0\n",
      "  (347, 136)\t1.0\n",
      "  (348, 122)\t1.0\n",
      "  (349, 70)\t1.0\n",
      "  (350, 122)\t1.0\n",
      "  (351, 54)\t1.0\n",
      "  (352, 115)\t1.0\n",
      "  (353, 73)\t1.0\n",
      "  (355, 124)\t1.0\n",
      "  (356, 54)\t1.0\n",
      "  (357, 54)\t1.0\n",
      "  (358, 59)\t1.0\n",
      "  (359, 132)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(wm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            12   15   17   18   19 2011 2012 2014 2016 2019   \n",
      "children.txt               NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \\\n",
      "kotonoha no niwa (id).txt  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "weathering.txt             NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "yourname.txt               NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "suzume (id).txt            NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "                           ... wilayah with wujud yakin yang yatim  you   \n",
      "children.txt               ...     NaN  NaN   NaN   NaN  NaN   NaN  NaN  \\\n",
      "kotonoha no niwa (id).txt  ...     NaN  NaN   NaN   NaN  NaN   NaN  NaN   \n",
      "weathering.txt             ...     NaN  NaN   NaN   NaN  NaN   NaN  NaN   \n",
      "yourname.txt               ...     NaN  NaN   NaN   NaN  NaN   NaN  NaN   \n",
      "suzume (id).txt            ...     NaN  NaN   NaN   NaN  NaN   NaN  NaN   \n",
      "\n",
      "                          yukari yukino zake  \n",
      "children.txt                 NaN    NaN  NaN  \n",
      "kotonoha no niwa (id).txt    NaN    NaN  NaN  \n",
      "weathering.txt               NaN    NaN  NaN  \n",
      "yourname.txt                 NaN    NaN  NaN  \n",
      "suzume (id).txt              NaN    NaN  NaN  \n",
      "\n",
      "[5 rows x 706 columns]\n"
     ]
    }
   ],
   "source": [
    "print(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/christinaandrea/Desktop/6th Semester/NLP/mini library/nlp.ipynb Cell 5\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mfor\u001b[39;00m terms,freq \u001b[39min\u001b[39;00m wordFreq\u001b[39m.\u001b[39mmost_common(\u001b[39m10\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             filedir[fileName][terms] \u001b[39m=\u001b[39m [freq][\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m tokenizedWords \u001b[39m=\u001b[39m regex\u001b[39m.\u001b[39;49mtokenize(final)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/christinaandrea/Desktop/6th%20Semester/NLP/mini%20library/nlp.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizedWords)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/tokenize/regexp.py:127\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gaps:\n\u001b[1;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_discard_empty:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m [tok \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_regexp\u001b[39m.\u001b[39;49msplit(text) \u001b[39mif\u001b[39;00m tok]\n\u001b[1;32m    128\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39msplit(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# lst = []\n",
    "# final = []\n",
    "# stemmer = StemmerFactory()\n",
    "# idn_stemmer = stemmer.create_stemmer()\n",
    "# for files in listFile:\n",
    "#     for i in listFile:\n",
    "#         files = open(dir+i,'r').read()\n",
    "#         splitText =  files.split(\"\\n\")\n",
    "#         lst.append(splitText)\n",
    "#     for x in lst:\n",
    "#         for i in x:\n",
    "#             content = idn_stemmer.stem(str.lower(i))\n",
    "#             final.append(content)\n",
    "    \n",
    "#     indoToken = []\n",
    "#     for words in tokenizedWords:\n",
    "#         if words not in stopword:\n",
    "#             indoToken.append(words)\n",
    "#     wordFreq = FreqDist(indoToken)\n",
    "\n",
    "#     for fileName in listFile:\n",
    "#         filedir[fileName] = {}  \n",
    "\n",
    "#         for terms,freq in wordFreq.most_common(10):\n",
    "#             filedir[fileName][terms] = [freq][0]\n",
    "\n",
    "\n",
    "# tokenizedWords = regex.tokenize(final)\n",
    "# print(tokenizedWords)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
